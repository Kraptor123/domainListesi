name: Domain Kontrol ve GÃ¼ncelleme

on:
  schedule:
    - cron: '0 */6 * * *'  # Her 6 saatte bir Ã§alÄ±ÅŸÄ±r (daha az sÄ±klÄ±kta)
  workflow_dispatch:  # Manuel tetikleme iÃ§in

jobs:
  domain-kontrolu:
    runs-on: ubuntu-latest
    timeout-minutes: 15  # Maksimum 15 dakika

    steps:
      - name: Depoyu klonla
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Python kurulumu
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Gerekli paketleri yÃ¼kle
        run: |
          pip install requests

      - name: Domainleri kontrol et ve gÃ¼ncelle
        env:
          PROXY_HOST: "176.235.182.90"
          PROXY_PORT: "1080"
        run: |
          python - <<'EOF'
          import re
          import requests
          import os
          from urllib.parse import urlparse
          from concurrent.futures import ThreadPoolExecutor, as_completed
          import time

          def get_final_url(url, timeout=8):
              """URL'nin son redirect edilen halini dÃ¶ndÃ¼rÃ¼r"""
              try:
                  url = url.strip().rstrip('/')
          
                  # Proxy ayarlarÄ± - opsiyonel
                  proxy_host = os.getenv('PROXY_HOST')
                  proxy_port = os.getenv('PROXY_PORT')
          
                  proxies = None
                  if proxy_host and proxy_port:
                      proxy_url = f"http://{proxy_host}:{proxy_port}"
                      proxies = {
                          'http': proxy_url,
                          'https': proxy_url
                      }
          
                  # Ã–nce proxy olmadan dene
                  try:
                      response = requests.get(
                          url, 
                          allow_redirects=True, 
                          timeout=timeout,
                          proxies=None,  # Ã–nce proxy'siz
                          headers={
                              'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                              'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
                          },
                          verify=True
                      )
          
                      final_url = response.url.rstrip('/')
                      parsed = urlparse(final_url)
                      clean_url = f"{parsed.scheme}://{parsed.netloc}"
          
                      return clean_url
          
                  except Exception as e1:
                      # Proxy ile tekrar dene
                      if proxies:
                          try:
                              response = requests.get(
                                  url, 
                                  allow_redirects=True, 
                                  timeout=timeout,
                                  proxies=proxies,
                                  headers={
                                      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                                      'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
                                  },
                                  verify=True
                              )
          
                              final_url = response.url.rstrip('/')
                              parsed = urlparse(final_url)
                              clean_url = f"{parsed.scheme}://{parsed.netloc}"
          
                              return clean_url
                          except:
                              pass
          
                      raise e1
          
              except Exception as e:
                  return None

          def parse_line(line):
              """SatÄ±rÄ± parse eder: |Name:https://domain.com"""
              match = re.match(r'\|([^:]+):(https?://[^\s]+)', line.strip())
              if match:
                  name = match.group(1)
                  url = match.group(2).rstrip('/')
                  return name, url
              return None, None

          def check_domain(name, url):
              """Tek bir domain'i kontrol eder"""
              print(f"Kontrol ediliyor: {name} ({url})")
          
              final_url = get_final_url(url)
          
              if final_url and final_url != url:
                  print(f"  âœ“ {name}: {url} â†’ {final_url}")
                  return name, url, final_url, True
              elif final_url:
                  print(f"  â†’ {name}: DeÄŸiÅŸiklik yok")
                  return name, url, url, False
              else:
                  print(f"  âœ— {name}: EriÅŸilemedi")
                  return name, url, url, False

          file_path = 'eklenti_domainleri.txt'
          
          try:
              with open(file_path, 'r', encoding='utf-8') as f:
                  content = f.read()
          except FileNotFoundError:
              print(f"Dosya bulunamadÄ±: {file_path}")
              exit(1)

          original_content = content
          changes_made = False
          updates = {}

          print("Domain kontrolleri baÅŸlÄ±yor...\n")

          lines = content.split('\n')
          domains_to_check = []
          
          # Kontrol edilecek domainleri topla
          for line in lines:
              if not line.strip() or not line.strip().startswith('|'):
                  continue
          
              name, url = parse_line(line)
              if name and url:
                  domains_to_check.append((name, url))

          # Paralel kontrol (max 5 eÅŸzamanlÄ±)
          with ThreadPoolExecutor(max_workers=5) as executor:
              future_to_domain = {
                  executor.submit(check_domain, name, url): (name, url) 
                  for name, url in domains_to_check
              }
          
              for future in as_completed(future_to_domain):
                  name, old_url, new_url, changed = future.result()
                  if changed:
                      updates[old_url] = new_url
                      changes_made = True

          print(f"\n{'='*50}")
          print(f"Toplam kontrol edilen: {len(domains_to_check)}")
          print(f"GÃ¼ncelleme gereken: {len(updates)}")
          print(f"{'='*50}\n")

          # GÃ¼ncellemeleri uygula
          if changes_made:
              for old_url, new_url in updates.items():
                  # SatÄ±r satÄ±r deÄŸiÅŸtir
                  lines = content.split('\n')
                  for i, line in enumerate(lines):
                      if old_url in line:
                          name, _ = parse_line(line)
                          if name:
                              lines[i] = f"|{name}:{new_url}"
                  content = '\n'.join(lines)

          # DosyayÄ± sadece deÄŸiÅŸiklik varsa gÃ¼ncelle
          if changes_made and content != original_content:
              with open(file_path, 'w', encoding='utf-8') as f:
                  f.write(content)
              print("âœ“ Dosya gÃ¼ncellendi!")
          
              # GÃ¼ncellenen domainleri listele
              print("\nGÃ¼ncellenen domainler:")
              for old_url, new_url in updates.items():
                  print(f"  {old_url} â†’ {new_url}")
          else:
              print("â†’ GÃ¼ncelleme gerekmedi.")

          EOF

      - name: DeÄŸiÅŸiklikleri kaydet ve yÃ¼kle
        run: |
          git config --global user.name 'Domain Kontrol Botu'
          git config --global user.email 'bot@github-actions'
          
          git add eklenti_domainleri.txt
          
          if git diff --staged --quiet; then
            echo "DeÄŸiÅŸiklik yok, commit atlanÄ±yor"
          else
            git pull --rebase origin main || git pull origin main
            git commit -m "ðŸ”„ Domain gÃ¼ncellemesi - $(date +'%Y-%m-%d %H:%M UTC')"
            git push
            echo "âœ“ DeÄŸiÅŸiklikler push edildi"
          fi